{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4W823_j7WZC",
        "outputId": "f9d7e0b9-3c2b-414c-f4bf-ca3ce70ae740"
      },
      "outputs": [],
      "source": [
        "!pip install datasets pandas pyarrow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-wzQKD59TWM"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiG_AHmv9UJQ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Directly stream the  Gujarati split\n",
        "gujarati_stream = load_dataset(\n",
        "    \"ai4bharat/IndicCorpV2\",\n",
        "    \"indiccorp_v2\",\n",
        "    streaming=True,\n",
        "    split=\"guj_Gujr\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6gye2vI-vdi"
      },
      "outputs": [],
      "source": [
        "def custom_tokenizer_guj_with_matras(text):\n",
        "    # Handle special items\n",
        "    url_pattern = r'https?://[^\\s]+|www\\.[^\\s]+'\n",
        "    email_pattern = r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,4}\\b'\n",
        "\n",
        "    urls = re.findall(url_pattern, text)\n",
        "    text = re.sub(url_pattern, '<URL>', text)\n",
        "\n",
        "    emails = re.findall(email_pattern, text)\n",
        "    text = re.sub(email_pattern, '<EMAIL>', text)\n",
        "\n",
        "    tokens = []\n",
        "    for word in text.split():\n",
        "        if word == '<URL>':\n",
        "            tokens.append(urls.pop(0))\n",
        "        elif word == '<EMAIL>':\n",
        "            tokens.append(emails.pop(0))\n",
        "        else:\n",
        "            # Tokenize gujarati characters while separating matras\n",
        "            word_tokens = re.findall(rf'[\\u0915-\\u0939][\\u093e-\\u094c\\u0902\\u0903]?|\\d+|[^\\s\\w]', word)\n",
        "            tokens.extend(word_tokens)\n",
        "\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQZSYWYO9ZrW"
      },
      "outputs": [],
      "source": [
        "from itertools import islice\n",
        "\n",
        "# Example: Get first 5 entries from the iterable\n",
        "samples = list(islice(gujarati_stream, 5))  # change 5 to any number\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gH9LlhDy_rkY"
      },
      "outputs": [],
      "source": [
        "text = samples[0]['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMCBzr1N9ccn"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def guj_tokenizer(text):\n",
        "    url_pattern = r'https?://[^\\s]+|www\\.[^\\s]+'\n",
        "    email_pattern = r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,4}\\b'\n",
        "\n",
        "    # Save and replace URLs and emails\n",
        "    urls = re.findall(url_pattern, text)\n",
        "    text = re.sub(url_pattern, '<URL>', text)\n",
        "\n",
        "    emails = re.findall(email_pattern, text)\n",
        "    text = re.sub(email_pattern, '<EMAIL>', text)\n",
        "\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token == '<URL>':\n",
        "            tokens.append(urls.pop(0))\n",
        "        elif token == '<EMAIL>':\n",
        "            tokens.append(emails.pop(0))\n",
        "        else:\n",
        "            split_tokens = re.findall(\n",
        "                r'[\\u0900-\\u097F]+|[a-zA-Z0-9]+|[।.,!?;:()\\\"\\'\\-]|[^\\s]',\n",
        "                token\n",
        "            )\n",
        "            tokens.extend(split_tokens)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def guj_sentence_tokenizer(text):\n",
        "    sentence_end_pattern = r'(?<=[।!?\\.])\\s+'\n",
        "    sentences = re.split(sentence_end_pattern, text.strip())\n",
        "    return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "def detokenize(tokens):\n",
        "    # Rebuild sentence with proper spacing logic\n",
        "    sentence = ''\n",
        "    for i, token in enumerate(tokens):\n",
        "        if i > 0 and not re.match(r'[।.,!?;:)\\]\\'\\\"]', token):\n",
        "            sentence += ' '\n",
        "        sentence += token\n",
        "    return sentence.strip()\n",
        "\n",
        "def guj_corpus_statistics(text):\n",
        "    sentences = guj_sentence_tokenizer(text)\n",
        "    all_tokens = []\n",
        "    reconstructed_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        tokens = guj_tokenizer(sentence)\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "        # For checking reformation\n",
        "        reconstructed = detokenize(tokens)\n",
        "        reconstructed_sentences.append(reconstructed)\n",
        "\n",
        "    num_tokens = len(all_tokens)\n",
        "    unique_tokens = set(all_tokens)\n",
        "    total_chars = sum(len(token) for token in all_tokens)\n",
        "\n",
        "    word_tokens = [t for t in all_tokens if re.match(r'^[\\u0900-\\u097F\\w]+$', t)]\n",
        "    avg_word_length = sum(len(t) for t in word_tokens) / len(word_tokens) if word_tokens else 0\n",
        "    type_token_ratio = len(unique_tokens) / num_tokens if num_tokens else 0\n",
        "\n",
        "    return {\n",
        "        'sentences': sentences,\n",
        "        'tokens': all_tokens,\n",
        "        'num_tokens': num_tokens,\n",
        "        'total_characters': total_chars,\n",
        "        'average_word_length': round(avg_word_length, 2),\n",
        "        'type_token_ratio': round(type_token_ratio, 3),\n",
        "        'reconstructed_sentences': reconstructed_sentences\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cRBr27H9eMH",
        "outputId": "b2918ca5-9473-4caa-929b-ccc8a2d9404f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Sentences:\n",
            "આ વીડિયો જુઓ: ઊંઝા માર્કેટયાર્ડ આજથી 25 જુલાઈ સુધી બંધ  મિથેનોલ આવ્યો ક્યાંથી?\n",
            "આખરે ત્રણ રાજ્યોમાં મળેલ હાર પર કોંગ્રેસ અધ્યક્ષ રાહુલ ગાંધી દ્વારા પ્રથમ પ્રતિક્રિયા આપવામાં આવી છે.\n",
            "તેમણે કહ્યું કે, ત્રિપુરા, નાગાલેન્ડ અને મેઘાલયમાં લોકોના જનાદેશનો સ્વાગત કરીએ છે અને આ ક્ષેત્રના લોકોનો વિશ્વાસ ફરીથી જીતીવા માટે પ્રતિબદ્ધ છીએ.\n",
            "\n",
            "Reconstructed Sentences:\n",
            "આ વ ી ડ િ ય ો જ ુ ઓ: ઊ ં ઝ ા મ ા ર ્ ક ે ટ ય ા ર ્ ડ આ જ થ ી 25 જ ુ લ ા ઈ સ ુ ધ ી બ ં ધ મ િ થ ે ન ો લ આ વ ્ ય ો ક ્ ય ા ં થ ી?\n",
            "આ ખ ર ે ત ્ ર ણ ર ા જ ્ ય ો મ ા ં મ ળ ે લ હ ા ર પ ર ક ો ં ગ ્ ર ે સ અ ધ ્ ય ક ્ ષ ર ા હ ુ લ ગ ા ં ધ ી દ ્ વ ા ર ા પ ્ ર થ મ પ ્ ર ત િ ક ્ ર િ ય ા આ પ વ ા મ ા ં આ વ ી છ ે.\n",
            "ત ે મ ણ ે ક હ ્ ય ુ ં ક ે, ત ્ ર િ પ ુ ર ા, ન ા ગ ા લ ે ન ્ ડ અ ન ે મ ે ઘ ા લ ય મ ા ં લ ો ક ો ન ા જ ન ા દ ે શ ન ો સ ્ વ ા ગ ત ક ર ી એ છ ે અ ન ે આ ક ્ ષ ે ત ્ ર ન ા લ ો ક ો ન ો વ િ શ ્ વ ા સ ફ ર ી થ ી જ ી ત ી વ ા મ ા ટ ે પ ્ ર ત િ બ દ ્ ધ છ ી એ.\n",
            "\n",
            "Tokens: ['આ', 'વ', 'ી', 'ડ', 'િ', 'ય', 'ો', 'જ', 'ુ', 'ઓ', ':', 'ઊ', 'ં', 'ઝ', 'ા', 'મ', 'ા', 'ર', '્', 'ક', 'ે', 'ટ', 'ય', 'ા', 'ર', '્', 'ડ', 'આ', 'જ', 'થ', 'ી', '25', 'જ', 'ુ', 'લ', 'ા', 'ઈ', 'સ', 'ુ', 'ધ', 'ી', 'બ', 'ં', 'ધ', 'મ', 'િ', 'થ', 'ે', 'ન', 'ો', 'લ', 'આ', 'વ', '્', 'ય', 'ો', 'ક', '્', 'ય', 'ા', 'ં', 'થ', 'ી', '?', 'આ', 'ખ', 'ર', 'ે', 'ત', '્', 'ર', 'ણ', 'ર', 'ા', 'જ', '્', 'ય', 'ો', 'મ', 'ા', 'ં', 'મ', 'ળ', 'ે', 'લ', 'હ', 'ા', 'ર', 'પ', 'ર', 'ક', 'ો', 'ં', 'ગ', '્', 'ર', 'ે', 'સ', 'અ', 'ધ', '્', 'ય', 'ક', '્', 'ષ', 'ર', 'ા', 'હ', 'ુ', 'લ', 'ગ', 'ા', 'ં', 'ધ', 'ી', 'દ', '્', 'વ', 'ા', 'ર', 'ા', 'પ', '્', 'ર', 'થ', 'મ', 'પ', '્', 'ર', 'ત', 'િ', 'ક', '્', 'ર', 'િ', 'ય', 'ા', 'આ', 'પ', 'વ', 'ા', 'મ', 'ા', 'ં', 'આ', 'વ', 'ી', 'છ', 'ે', '.', 'ત', 'ે', 'મ', 'ણ', 'ે', 'ક', 'હ', '્', 'ય', 'ુ', 'ં', 'ક', 'ે', ',', 'ત', '્', 'ર', 'િ', 'પ', 'ુ', 'ર', 'ા', ',', 'ન', 'ા', 'ગ', 'ા', 'લ', 'ે', 'ન', '્', 'ડ', 'અ', 'ન', 'ે', 'મ', 'ે', 'ઘ', 'ા', 'લ', 'ય', 'મ', 'ા', 'ં', 'લ', 'ો', 'ક', 'ો', 'ન', 'ા', 'જ', 'ન', 'ા', 'દ', 'ે', 'શ', 'ન', 'ો', 'સ', '્', 'વ', 'ા', 'ગ', 'ત', 'ક', 'ર', 'ી', 'એ', 'છ', 'ે', 'અ', 'ન', 'ે', 'આ', 'ક', '્', 'ષ', 'ે', 'ત', '્', 'ર', 'ન', 'ા', 'લ', 'ો', 'ક', 'ો', 'ન', 'ો', 'વ', 'િ', 'શ', '્', 'વ', 'ા', 'સ', 'ફ', 'ર', 'ી', 'થ', 'ી', 'જ', 'ી', 'ત', 'ી', 'વ', 'ા', 'મ', 'ા', 'ટ', 'ે', 'પ', '્', 'ર', 'ત', 'િ', 'બ', 'દ', '્', 'ધ', 'છ', 'ી', 'એ', '.']\n",
            "Number of Tokens: 274\n",
            "Total Characters: 275\n",
            "Average Word Length: 1.01\n",
            "Type-Token Ratio: 0.172\n"
          ]
        }
      ],
      "source": [
        "text = ' '.join(sample['text'] for sample in samples)\n",
        "stats = guj_corpus_statistics(text)\n",
        "\n",
        "print(\"Original Sentences:\")\n",
        "for sentence in stats['sentences']:\n",
        "    print(sentence)\n",
        "\n",
        "print(\"\\nReconstructed Sentences:\")\n",
        "for sent in stats['reconstructed_sentences']:\n",
        "    print(sent)\n",
        "\n",
        "print(\"\\nTokens:\", stats['tokens'])\n",
        "print(\"Number of Tokens:\", stats['num_tokens'])\n",
        "print(\"Total Characters:\", stats['total_characters'])\n",
        "print(\"Average Word Length:\", stats['average_word_length'])\n",
        "print(\"Type-Token Ratio:\", stats['type_token_ratio'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPDMUpXgAk6a"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def english_tokenizer(text):\n",
        "    # Define patterns\n",
        "    url_pattern = r'https?://[^\\s]+|www\\.[^\\s]+'\n",
        "    email_pattern = r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,}\\b'\n",
        "\n",
        "    # Save and replace URLs/emails\n",
        "    urls = re.findall(url_pattern, text)\n",
        "    text = re.sub(url_pattern, '<URL>', text)\n",
        "\n",
        "    emails = re.findall(email_pattern, text)\n",
        "    text = re.sub(email_pattern, '<EMAIL>', text)\n",
        "\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token == '<URL>':\n",
        "            tokens.append(urls.pop(0))\n",
        "        elif token == '<EMAIL>':\n",
        "            tokens.append(emails.pop(0))\n",
        "        else:\n",
        "            # Tokenize: words, numbers, punctuation\n",
        "            split_tokens = re.findall(r\"[a-zA-Z0-9]+|[.,!?;:'\\\"()\\-]|[^\\s]\", token)\n",
        "            tokens.extend(split_tokens)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def english_sentence_tokenizer(text):\n",
        "    # Split on sentence-ending punctuation followed by space\n",
        "    sentence_end_pattern = r'(?<=[.!?])\\s+'\n",
        "    sentences = re.split(sentence_end_pattern, text.strip())\n",
        "    return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "def detokenize(tokens):\n",
        "    sentence = ''\n",
        "    for i, token in enumerate(tokens):\n",
        "        if i > 0 and not re.match(r'[.,!?;:)\\]\\'\\\"]', token):\n",
        "            sentence += ' '\n",
        "        sentence += token\n",
        "    return sentence.strip()\n",
        "\n",
        "def english_corpus_statistics(text):\n",
        "    sentences = english_sentence_tokenizer(text)\n",
        "    all_tokens = []\n",
        "    reconstructed_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        tokens = english_tokenizer(sentence)\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "        reconstructed = detokenize(tokens)\n",
        "        reconstructed_sentences.append(reconstructed)\n",
        "\n",
        "    num_tokens = len(all_tokens)\n",
        "    unique_tokens = set(all_tokens)\n",
        "    total_chars = sum(len(token) for token in all_tokens)\n",
        "\n",
        "    word_tokens = [t for t in all_tokens if re.match(r'^[a-zA-Z0-9]+$', t)]\n",
        "    avg_word_length = sum(len(t) for t in word_tokens) / len(word_tokens) if word_tokens else 0\n",
        "    type_token_ratio = len(unique_tokens) / num_tokens if num_tokens else 0\n",
        "\n",
        "    return {\n",
        "        'sentences': sentences,\n",
        "        'tokens': all_tokens,\n",
        "        'num_tokens': num_tokens,\n",
        "        'total_characters': total_chars,\n",
        "        'average_word_length': round(avg_word_length, 2),\n",
        "        'type_token_ratio': round(type_token_ratio, 3),\n",
        "        'reconstructed_sentences': reconstructed_sentences\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTmHGrXQBOM8",
        "outputId": "412f7411-a094-484a-c3c9-51565d2db2b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Sentences:\n",
            "Hello!\n",
            "I'm Alex.\n",
            "Feel free to drop a message at alex.jordan@gmail.com or visit my blog at www.alexwrites.com.\n",
            "Hope you find it interesting!\n",
            "\n",
            "Reconstructed Sentences:\n",
            "Hello!\n",
            "I' m Alex.\n",
            "Feel free to drop a message at alex.jordan@gmail.com or visit my blog at www.alexwrites.com.\n",
            "Hope you find it interesting!\n",
            "\n",
            "Tokens: ['Hello', '!', 'I', \"'\", 'm', 'Alex', '.', 'Feel', 'free', 'to', 'drop', 'a', 'message', 'at', 'alex.jordan@gmail.com', 'or', 'visit', 'my', 'blog', 'at', 'www.alexwrites.com.', 'Hope', 'you', 'find', 'it', 'interesting', '!']\n",
            "Number of Tokens: 27\n",
            "Total Characters: 118\n",
            "Average Word Length: 3.52\n",
            "Type-Token Ratio: 0.926\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"Hello! I'm Alex. Feel free to drop a message at alex.jordan@gmail.com or visit my blog at www.alexwrites.com. Hope you find it interesting!\"\"\"\n",
        "\n",
        "stats = english_corpus_statistics(text)\n",
        "\n",
        "print(\"Original Sentences:\")\n",
        "for s in stats['sentences']:\n",
        "    print(s)\n",
        "\n",
        "print(\"\\nReconstructed Sentences:\")\n",
        "for s in stats['reconstructed_sentences']:\n",
        "    print(s)\n",
        "\n",
        "print(\"\\nTokens:\", stats['tokens'])\n",
        "print(\"Number of Tokens:\", stats['num_tokens'])\n",
        "print(\"Total Characters:\", stats['total_characters'])\n",
        "print(\"Average Word Length:\", stats['average_word_length'])\n",
        "print(\"Type-Token Ratio:\", stats['type_token_ratio'])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
