{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KJ44Uwn2WPkO"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import random\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Load and Split the Dataset\n",
        "\n",
        "with open(\"/content/Guj_3000.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    sentences = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "random.shuffle(sentences)\n",
        "\n",
        "# Create splits\n",
        "val_set = sentences[:1000]\n",
        "test_set = sentences[1000:2000]\n",
        "train_set = sentences[2000:]\n",
        "\n",
        "print(f\"Train: {len(train_set)}, Val: {len(val_set)}, Test: {len(test_set)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9SCm7ewXxd6",
        "outputId": "3128efc1-0931-47bf-ef76-9ded2c4f122b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 1000, Val: 1000, Test: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. Preprocessing\n",
        "\n",
        "def tokenize(text):\n",
        "    return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "train_tokens = [tokenize(s) for s in train_set]\n",
        "val_tokens = [tokenize(s) for s in val_set]\n",
        "test_tokens = [tokenize(s) for s in test_set]\n",
        "\n",
        "# Vocabulary\n",
        "vocab = set([w for sent in train_tokens for w in sent])\n",
        "V = len(vocab)\n",
        "print(\"Vocabulary Size:\", V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWY7heV0X7Q7",
        "outputId": "331c79c7-869d-40f6-9812-73114c633fdd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3. Build N-gram Models\n",
        "\n",
        "def build_ngrams(tokens_list, n):\n",
        "    counts = Counter()\n",
        "    for sent in tokens_list:\n",
        "        sent = [\"<s>\"]*(n-1) + sent + [\"</s>\"]\n",
        "        for i in range(len(sent)-n+1):\n",
        "            ngram = tuple(sent[i:i+n])\n",
        "            counts[ngram] += 1\n",
        "    return counts\n",
        "\n",
        "unigrams = build_ngrams(train_tokens, 1)\n",
        "bigrams = build_ngrams(train_tokens, 2)\n",
        "trigrams = build_ngrams(train_tokens, 3)\n",
        "quadgrams = build_ngrams(train_tokens, 4)\n",
        "\n",
        "print(\"Example 5 unigrams:\", list(unigrams.items())[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9IUT_PPYBZr",
        "outputId": "c1c057b8-c8b4-4975-a506-aadde72bd4d9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 5 unigrams: [(('529',), 2), (('આણ',), 26), (('દમ',), 121), (('હ',), 719), (('લમ',), 242)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 4. Good Turing Smoothing\n",
        "\n",
        "def good_turing_probs(ngram_counts, n, V):\n",
        "    N = sum(ngram_counts.values())\n",
        "    Nc = Counter(ngram_counts.values())\n",
        "    N1 = Nc[1]\n",
        "\n",
        "    # Seen probabilities\n",
        "    probs = {}\n",
        "    for ng, c in ngram_counts.items():\n",
        "        c_star = ( (c+1) * (Nc[c+1]/Nc[c]) ) if (c+1) in Nc else c\n",
        "        probs[ng] = c_star / N\n",
        "\n",
        "    # Unseen probability\n",
        "    total_seen = len(ngram_counts)\n",
        "    if n == 1:\n",
        "        unseen_prob = N1 / N / (V - len(unigrams))\n",
        "    else:\n",
        "        unseen_prob = N1 / N / ((V**n) - total_seen)\n",
        "\n",
        "    return probs, unseen_prob\n",
        "\n",
        "uni_probs, uni_unseen = good_turing_probs(unigrams, 1, V)\n",
        "bi_probs, bi_unseen = good_turing_probs(bigrams, 2, V)\n",
        "tri_probs, tri_unseen = good_turing_probs(trigrams, 3, V)\n",
        "quad_probs, quad_unseen = good_turing_probs(quadgrams, 4, V)"
      ],
      "metadata": {
        "id": "lVVSzHxIZXsG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 5. Sentence Probability\n",
        "\n",
        "def sentence_prob(tokens, probs, unseen_prob, n):\n",
        "    tokens = [\"<s>\"]*(n-1) + tokens + [\"</s>\"]\n",
        "    log_prob = 0\n",
        "    for i in range(len(tokens)-n+1):\n",
        "        ng = tuple(tokens[i:i+n])\n",
        "        p = probs.get(ng, unseen_prob)\n",
        "        log_prob += math.log(p)\n",
        "    return log_prob\n",
        "\n",
        "# Example validation computation\n",
        "print(\"Sentence Probability Example:\")\n",
        "print(val_set[0])\n",
        "print(sentence_prob(tokenize(val_set[0]), bi_probs, bi_unseen, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8hGh-tTZeuy",
        "outputId": "f1400905-0649-4005-f17f-e97d3d74f8f3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Probability Example:\n",
            "459. ભુજમાં આ મહિને વિજ્ઞાન ક્ષેત્રે મેટ્રોલોજીકલ સેન્ટરએ અહેવાલ બહાર પાડ્યો; પછી 23 મુદ્દાઓ પર ભાર મૂકાયો અને જનજાગૃતિ અભિયાન તેજ બન્યું. પ્રોજેક્ટ માટે ૨૦ કરોડ રૂપિયાની ફાળવણી કરવામાં આવી છે.\n",
            "-478.90136008304427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 6. Deleted Interpolation (Quadrigram)\n",
        "\n",
        "def deleted_interpolation(train_tokens):\n",
        "    lambdas = [0.25, 0.25, 0.25, 0.25]  # initial equal weights\n",
        "    # TODO: implement EM algorithm to tune lambdas on held-out data\n",
        "    return lambdas\n",
        "\n",
        "lambdas = deleted_interpolation(train_tokens)\n",
        "print(\"Interpolation weights:\", lambdas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2viLYsw7ZkPz",
        "outputId": "5eadbaa6-203c-40cc-d1f3-141131a7ea3e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interpolation weights: [0.25, 0.25, 0.25, 0.25]\n"
          ]
        }
      ]
    }
  ]
}